MAX_EPR_PAIR: 9
MAX_GHZ_PAIR: 9
COOLDOWN_SWAP: 3
COOLDOWN_GENERATE: 5
COOLDOWN_GHZGENERATE: 5
COOLDOWN_SCORE: 1
COOLDOWN_TELE_GATE: 5
COOLDOWN_TELE_QUBIT: 5
COOLDOWN_TELE_TOFFOLI: 5
ENTANGLEMENT_PROBABILITY: 0.95
REWARD_STOP: -2
REWARD_FOR_SWAP: 0
REWARD_SCORE: 50
REWARD_SCORE_TOFFOLI: 60
REWARD_EMPTY_DAG: 300
REWARD_DEADLINE: -300
NUMQ: 10
NUMG: 10
TOFOLI_GATE_PROB: 0.1
DISTANCE_MULT: 18
DISTANCE_QUANTUM_LINK: 30
DISTANCE_BETWEEN_EPR: 1
QPU_Type:
- Guadalupe
- Guadalupe
- Guadalupe
IF_ENABLE_TOFFOLIOS: true
USE_DEC_DEDLINE: false
USE_DSF_MAPPING: false
COMPLETION_DEADLINE: 1000
result_path: running_result/run_on_10Q_10G_with_tofoli02
seed: 7
num_episodes_to_run: 10
file_to_save_data_results: results/data_and_graphs/dist_quantum_Results_Data.pkl
file_to_save_results_graph: results/data_and_graphs/dist_quantum__Results_Graph.png
show_solution_score: true
visualise_individual_results: false
visualise_overall_agent_results: true
baselines: true
standard_deviation_results: 1.0
runs_per_agent: 1
use_GPU: true
overwrite_existing_results_file: false
randomise_random_seed: true
save_model: true
AGENTS:
- DQN
hyperparameters:
  DQN_Agents:
    learning_rate: 1.0e-05
    batch_size: 2560
    buffer_size: 100000
    epsilon: 1.0
    epsilon_decay_rate_denominator: 80
    discount_rate: 0.99
    tau: 0.001
    update_every_n_steps: 5
    linear_hidden_units:
    - 140
    - 150
    final_layer_activation: None
    batch_norm: false
    gradient_clipping_norm: 0.7
    learning_iterations: 10
    clip_rewards: false
  Stochastic_Policy_Search_Agents:
    policy_network_type: Linear
    noise_scale_start: 0.01
    noise_scale_min: 0.001
    noise_scale_max: 2.0
    noise_scale_growth_factor: 2.0
    stochastic_action_decision: false
    num_policies: 10
    episodes_per_policy: 1
    num_policies_to_keep: 5
    clip_rewards: false
  Policy_Gradient_Agents:
    learning_rate: 5.0e-05
    linear_hidden_units:
    - 70
    - 180
    - 70
    final_layer_activation: SOFTMAX
    learning_iterations_per_round: 20
    discount_rate: 0.992
    batch_norm: false
    clip_epsilon: 0.2
    episodes_per_learning_round: 30
    normalise_rewards: true
    gradient_clipping_norm: 7.0
    mu: 0.0
    theta: 0.0
    sigma: 0.0
    epsilon_decay_rate_denominator: 50
    clip_rewards: false
  Actor_Critic_Agents:
    learning_rate: 0.005
    linear_hidden_units:
    - 70
    - 80
    - 70
    final_layer_activation:
    - SOFTMAX
    - null
    gradient_clipping_norm: 5.0
    discount_rate: 0.99
    epsilon_decay_rate_denominator: 1.0
    normalise_rewards: true
    exploration_worker_difference: 2.0
    clip_rewards: false
    Actor:
      learning_rate: 0.0003
      linear_hidden_units:
      - 64
      - 64
      - 64
      final_layer_activation: Softmax
      batch_norm: false
      tau: 0.005
      gradient_clipping_norm: 5
      initialiser: Xavier
    Critic:
      learning_rate: 0.0003
      linear_hidden_units:
      - 64
      - 64
      - 64
      final_layer_activation: null
      batch_norm: false
      buffer_size: 1000000
      tau: 0.005
      gradient_clipping_norm: 5
      initialiser: Xavier
    min_steps_before_learning: 400
    batch_size: 256
    mu: 0.0
    theta: 0.15
    sigma: 0.25
    action_noise_std: 0.2
    action_noise_clipping_range: 0.5
    update_every_n_steps: 1
    learning_updates_per_learning_session: 4
    automatically_tune_entropy_hyperparameter: true
    entropy_term_weight: null
    add_extra_noise: false
    do_evaluation_iterations: true
